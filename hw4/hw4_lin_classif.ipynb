{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baefb231-f040-4175-a48f-b4db88c8ea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Downloading scipy-1.11.3-cp310-cp310-macosx_10_9_x86_64.whl (37.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy<1.28.0,>=1.21.6\n",
      "  Using cached numpy-1.26.1-cp310-cp310-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "Installing collected packages: numpy, scipy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.1\n",
      "    Uninstalling numpy-1.26.1:\n",
      "      Successfully uninstalled numpy-1.26.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.0\n",
      "    Uninstalling scipy-1.10.0:\n",
      "      Successfully uninstalled scipy-1.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.1 scipy-1.11.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --force-reinstall scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a234a36-6f53-461a-ba6a-44c4322359e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__header__', '__version__', '__globals__', 'y', 'X']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "in_data = loadmat('face_emotion_data.mat')\n",
    "print([key for key in in_data]) # -- use this line to see the keys in the dictionary data structure \n",
    "\n",
    "# m(faces) x n(features) = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcfb2d1c-f8ac-4326-b4c9-223313e7d857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.94366942],\n",
       "       [ 0.21373778],\n",
       "       [ 0.26641775],\n",
       "       [-0.39221373],\n",
       "       [-0.00538552],\n",
       "       [-0.01764687],\n",
       "       [-0.16632809],\n",
       "       [-0.0822838 ],\n",
       "       [-0.16644364]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1a)\n",
    "y=in_data[\"y\"]\n",
    "X=in_data[\"X\"]\n",
    "# Using the formula: w = (X^T X)^(-1) X^T y\n",
    "X_transpose = in_data[\"X\"].transpose()\n",
    "w = np.linalg.inv(X.transpose()@ X) @ X.transpose() @ y\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9aae8ff-928a-41b8-a50c-9a16f69e6464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1b)\n",
    "# Each weight corresponds to one of the 9 features the model takes\n",
    "# when we solve y=X^Tw, each weight will be applied to its associated measurement for all 128 faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5915137a-07b9-4433-9bfc-d25e70621eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1c)\n",
    "#The features that seem to be most important are the ones who have the highest associated weights. Especially since\n",
    "#all the features have been normalized, they are on the same scale. Features x1, x3 and x4 seem to be the most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd39ed7b-71e9-44b2-9b0f-4682fca628d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70546316],\n",
       "       [ 0.8737872 ],\n",
       "       [-0.78805643]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1d)\n",
    "selected_columns = [0, 2, 3]\n",
    "x_slice = X[:, selected_columns]\n",
    "w2 = np.linalg.inv(x_slice.transpose()@ x_slice) @ x_slice.transpose() @ y\n",
    "#If we are minimizing the features we want to use we should include the ones that have the most importance. These being\n",
    "#the three expressed above\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3fda712-cd46-495b-9f93-fb24dc724afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent error for 9 features: 2.34%. Percent error for 3 features: 6.25%.\n"
     ]
    }
   ],
   "source": [
    "#1e)\n",
    "y_hat1 = np.sign(X@w)\n",
    "y_hat2 = np.sign(x_slice@w2)\n",
    "\n",
    "error_vec1 = [0 if i[0]==i[1] else 1 for i in np.hstack((y_hat1, y))]\n",
    "error_vec2 = [0 if i[0]==i[1] else 1 for i in np.hstack((y_hat2, y))]\n",
    "\n",
    "print(\"Percent error for 9 features: {}%. Percent error for 3 features: {}%.\".format(round(sum(error_vec1)/128*100, 2), round(sum(error_vec2)/128*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cd9b4a0-7d3f-4be9-923f-7b8b92b091c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Error Rate: 2.34%.\n"
     ]
    }
   ],
   "source": [
    "#1f)\n",
    "num_subsets = 8\n",
    "subset_size = len(X) // num_subsets\n",
    "error_rates = []\n",
    "\n",
    "#8folds cross validation\n",
    "for fold in range(num_subsets):\n",
    "    start_index = fold * subset_size\n",
    "    end_index = (fold + 1) * subset_size\n",
    "\n",
    "    X_train = np.concatenate((X[:start_index], X[end_index:]), axis=0)\n",
    "    y_train = np.concatenate((y[:start_index], y[end_index:]), axis=0)\n",
    "    X_holdout = X[start_index:end_index]\n",
    "    y_holdout = y[start_index:end_index]\n",
    "\n",
    "    predictions = X_holdout @ w\n",
    "    misclassifications = np.sum(np.sign(predictions) != y_holdout)\n",
    "    error_rate = misclassifications / len(X_holdout)\n",
    "    error_rates.append(error_rate)\n",
    "\n",
    "\n",
    "average_error_rate = np.mean(error_rates)\n",
    "print(\"Average Error Rate: {}%.\".format(round(average_error_rate*100, 2)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8979f2-9883-4832-8d09-61f29ee0834b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
